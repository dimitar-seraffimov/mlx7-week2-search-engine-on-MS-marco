{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import requests\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text: str) -> list[str]:\n",
    "    text = text.lower()\n",
    "    replacements = {\n",
    "        '.': ' <PERIOD> ',\n",
    "        ',': ' <COMMA> ', \n",
    "        '\"': ' <QUOTATION_MARK> ',\n",
    "        ';': ' <SEMICOLON> ', \n",
    "        '!': ' <EXCLAMATION_MARK> ', \n",
    "        '?': ' <QUESTION_MARK> ',\n",
    "        '(': ' <LEFT_PAREN> ', \n",
    "        ')': ' <RIGHT_PAREN> ', \n",
    "        '--': ' <HYPHENS> ', \n",
    "        ':': ' <COLON> '\n",
    "    }\n",
    "    for k, v in replacements.items():\n",
    "        text = text.replace(k, v)\n",
    "    words = text.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and load text8\n",
    "r = requests.get('https://huggingface.co/datasets/ardMLX/text8/resolve/main/text8')\n",
    "with open('text8', 'wb') as f: f.write(r.content)\n",
    "with open('text8') as f: text8 = f.read()\n",
    "\n",
    "text8_tokens = preprocess(text8)\n",
    "with open('dataprep/combined_vocab/text8_corpus.pkl', 'wb') as f: pickle.dump(text8_tokens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and preprocess MS MARCO combined dataset\n",
    "df = pd.read_parquet(\"dataprep/ms_marco_combined/combined.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MS MARCO: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 102023/102023 [00:39<00:00, 2580.07it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# extract and preprocess queries and passages\n",
    "query_tokens = []\n",
    "passage_tokens = []\n",
    "\n",
    "for _, row in tqdm.tqdm(df.iterrows(), total=len(df), desc=\"Processing MS MARCO\"):\n",
    "    query_tokens.extend(preprocess(str(row[\"query\"])))\n",
    "    for passage in row[\"passages\"][\"passage_text\"]:\n",
    "        passage_tokens.extend(preprocess(str(passage)))\n",
    "\n",
    "with open(\"dataprep/combined_vocab/ms_marco_queries.pkl\", \"wb\") as f: pickle.dump(query_tokens, f)\n",
    "with open(\"dataprep/combined_vocab/ms_marco_passages.pkl\", \"wb\") as f: pickle.dump(passage_tokens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined corpus created with 86,128,145 tokens\n"
     ]
    }
   ],
   "source": [
    "# build combined corpus and single vocabulary\n",
    "combined_corpus = text8_tokens + query_tokens + passage_tokens\n",
    "with open(\"dataprep/combined_vocab/combined_corpus.pkl\", \"wb\") as f: pickle.dump(combined_corpus, f)\n",
    "\n",
    "print(f\"Combined corpus created with {len(combined_corpus):,} tokens\")\n",
    "\n",
    "# count global frequencies and filter\n",
    "word_counts = Counter(combined_corpus)\n",
    "filtered_corpus = [word for word in combined_corpus if word_counts[word] > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort vocab and create lookup tables\n",
    "sorted_vocab = sorted(set(filtered_corpus), key=lambda w: word_counts[w], reverse=True)\n",
    "int_to_vocab = {idx + 1: word for idx, word in enumerate(sorted_vocab)}\n",
    "int_to_vocab[0] = \"<PAD>\"\n",
    "vocab_to_int = {word: idx for idx, word in int_to_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add <UNK> token\n",
    "unk_id = len(vocab_to_int)\n",
    "vocab_to_int[\"<UNK>\"] = unk_id\n",
    "int_to_vocab[unk_id] = \"<UNK>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary created with 150,164 unique tokens (including <PAD> and <UNK>)\n"
     ]
    }
   ],
   "source": [
    "# save vocab\n",
    "with open(\"dataprep/combined_vocab/tkn_vocab_to_int.pkl\", \"wb\") as f: pickle.dump(vocab_to_int, f)\n",
    "with open(\"dataprep/combined_vocab/tkn_int_to_vocab.pkl\", \"wb\") as f: pickle.dump(int_to_vocab, f)\n",
    "\n",
    "print(f\"Vocabulary created with {len(vocab_to_int):,} unique tokens (including <PAD> and <UNK>)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown examples: ['lahontan', 'septentrionale', 'anarchiste', 'propri', 'mutuellisme', 'amoralism', 'individualistically', 'experimenal', 'yarros', 'signficiant']\n",
      "Total unknowns: 957,481\n"
     ]
    }
   ],
   "source": [
    "unk_words = [word for word in combined_corpus if word not in vocab_to_int]\n",
    "print(\"Unknown examples:\", unk_words[:10])\n",
    "print(f\"Total unknowns: {len(unk_words):,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
